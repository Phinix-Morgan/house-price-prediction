# -*- coding: utf-8 -*-
"""house_prediction_kaggle.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xeNjNvXV_y8NC3d9NgS5XwOnyIViSXHK

# Step 1: Set up the environment and import libraries
"""

import pandas as pd
import tensorflow as tf
import numpy as np
from tabulate import tabulate
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

"""# Step 2: Load data and Explore"""

melbourne_data =pd.read_csv("house_price_regression_dataset.csv")

pd.options.display.float_format = "{:,.2f}".format
print(melbourne_data.head())

"""**More Options :**

```


#Ensure House_Price remains a float  
df["House_Price"] = df["House_Price"].replace(',', '', regex=True).astype(float)

#Apply formatting for better readability  
#Keeps float, just changes display    
pd.options.display.float_format = "{:,.2f}".format

#Reset display format (or use scientific notation explicitly)    
#Default format (scientific notation for large numbers)    
pd.reset_option("display.float_format")

print(melbourne_data.head())


```
"""

melbourne_data.head()

melbourne_data.info()

melbourne_data.columns

"""**Checking For Missing Values :**"""

# Count missing values per column
filtered_melbourne_data=melbourne_data.isnull().sum()
print(filtered_melbourne_data)

"""# Step 3: Preprocess Data

SELECT FEATURES AND TARGET
"""

# Choose target and features
y = melbourne_data.House_Price

# selecting features
melbourne_features = ['Square_Footage', 'Num_Bedrooms', 'Year_Built','Lot_Size', 'Garage_Size']

x = melbourne_data[melbourne_features]

"""Split into Training & Testing Sets"""

# Split into Training & Testing Sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)

print("# VALIDATION FEATURES DATA")
print("\n",x_test[:10])

print("\n# VALIDATION TARGET DATA\n")
print(y_test[:10])

"""# Step 4: Train, Predict And Validate

**Train**
"""

# Define model
melbourne_model =RandomForestRegressor(random_state=1)
# Fit model
melbourne_model.fit(x_train, y_train)

"""**Make Predictions**

**Method 1:**
"""

# Predicte price on validation data
val_predictions = melbourne_model.predict(x_test)
print(val_predictions[:5])  # Not necessarily needed

"""**Method 2 :**
```
zxn=val_predictions[:5]
for i in range(len(zxn)):
    print(zxn[i])
    
OUTPUT >>>
475471.1766686792
405940.62394306617
344363.2736819858
926488.454118578
838719.430503504

```

# Step 5: Compare Actual vs Predicted Prices

**METHOD 1 :**
"""

df_results = pd.DataFrame({"Actual": y_test[0:10], "Predicted": val_predictions[0:10]})

df_results = tabulate(df_results, headers='keys', tablefmt='psql')

print(df_results)

"""**Method 2:**


```
df_results = pd.DataFrame({"Actual": y_test[:5], "Predicted": val_predictions[:5]})
print(df_results)

OUTPUT >>
      Actual       Predicted
507   501,692.85   475,471.18
818   398,823.27   405,940.62
452   302,975.30   344,363.27
368   874,856.27   926,488.45
242   811,129.50   838,719.43
```

#Step 6:  Evaluate the Model

**VALIDATE**

**✔ Use Evaluation Metrics:**
"""

mae = mean_absolute_error(y_test, val_predictions)
mse = mean_squared_error(y_test, val_predictions)
r2 = r2_score(y_test, val_predictions)

print(f"Mean Absolute Error: {mae}\n")
print(f"Mean Squared Error: {mse}\n")

# Closer to 1 means a better model --- [exception : overfitting]
print(f"R² Score: {r2}\n")

"""**TESTING ON NEW DATA**"""

# Your new house data as a NumPy array
new_house_x = np.array([[4615,4,2000,1.7211468359255475,1]])

# Define the same feature names as used during training
feature_names_x = ['Square_Footage',"Num_Bedrooms","Year_Built",'Lot_Size','Garage_Size']

# Convert the NumPy array to a DataFrame with the correct column names
new_house_df_x = pd.DataFrame(new_house_x, columns=feature_names_x)

# Use the DataFrame for prediction
predicted_price_x = melbourne_model.predict(new_house_df_x)
print(f"Predicted Price: ${predicted_price_x[0]:,.2f}")

"""# OPTIMIZING THE MODEL

**Controlling The Tree Depth**

**control overfitting vs underfitting using max_leaf_nodes.**

**Finding Best Tree Size**
"""

def get_mae(max_leaf_nodes, x_train, x_test, y_train, y_test):
    model = RandomForestRegressor(max_leaf_nodes=max_leaf_nodes, random_state=1)
    model.fit(x_train, y_train)
    preds_val = model.predict(x_test)
    mae = mean_absolute_error(y_test, preds_val)
    return mae

# Different values of max_leaf_nodes to test
max_nodes = [128,129,130,131,132,133]

scores = {}  # Initialize an empty dictionary

# Compare MAE with different max_leaf_nodes values
for leaf_size in max_nodes:
    mae = get_mae(leaf_size, x_train, x_test, y_train, y_test)  # Compute MAE
    scores[leaf_size] = mae  # Store in dictionary
    print(f"Max leaf nodes: {leaf_size:3d}  \t Mean Absolute Error: {mae:.2f}")

# Find the best value of max_leaf_nodes
best_tree_size = min(scores, key=scores.get)
print(f"\nBest Tree Size: {best_tree_size}")

"""# Train, Predict And Validate

**TRAIN**
"""

# Specifying final model
final_model = RandomForestRegressor(max_leaf_nodes=best_tree_size, random_state=1)
# fit the final model
final_model.fit(x_train, y_train)

"""**PREDICT**"""

# Predicte price on validation data
final_val_predictions = final_model.predict(x_test)
print(final_val_predictions[:5])

"""**COMPARE WITH ACTUAL PRICE**"""

final_df_results = pd.DataFrame({"Actual": y_test[0:10], "Predicted": final_val_predictions[0:10]})

final_df_results = tabulate(final_df_results, headers='keys', tablefmt='psql')

print(final_df_results)

"""**VALIDATE**"""

mae = mean_absolute_error(y_test, final_val_predictions)
mse = mean_squared_error(y_test, final_val_predictions)
r2 = r2_score(y_test, final_val_predictions)

print(f"Mean Absolute Error: {mae}")
print(f"Mean Squared Error: {mse}")

# Closer to 1 means a better model --- [exception : overfitting]
print(f"R² Score: {r2}")

"""#Step 8: Test with New Data

```
np.array :
#----------------------------------------------------------------#
new_house = np.array([[1700, 3, 2020]])  
predicted_price = model.predict(new_house)
print(f"Predicted Price: ${predicted_price[0]:,.2f}")
# ---------------------------------------------------------------#
```
"""

# Your new house data as a NumPy array
new_house = np.array([[4615,4,2000,1.7211468359255475,1]])

# Define the same feature names as used during training
feature_names = ['Square_Footage',"Num_Bedrooms","Year_Built",'Lot_Size','Garage_Size']

# Convert the NumPy array to a DataFrame with the correct column names
new_house_df = pd.DataFrame(new_house, columns=feature_names)

# Use the DataFrame for prediction
predicted_price = final_model.predict(new_house_df)
print(f"Predicted Price: ${predicted_price[0]:,.2f}")

"""#Step 9: Save the Model in Colab

**For a Pickle model (.pkl):**
"""

import pickle

# Save model
with open("house_price_model.pkl", "wb") as f:
    pickle.dump(final_model, f)

"""#Step 10: Download the File to Your PC"""

from google.colab import files
files.download("house_price_model.pkl")  # For Pickle
# OR
#files.download("house_price_model.h5")   # For Keras model